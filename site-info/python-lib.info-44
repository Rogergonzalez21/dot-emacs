This is python-lib.info, produced by makeinfo version 4.3 from
python-lib.texi.

October 3, 2003


File: python-lib.info,  Node: Information Discovery,  Prev: Emulation of compile,  Up: Examples 12

Information Discovery
.....................

Some applications benefit from direct access to the parse tree.  The
remainder of this section demonstrates how the parse tree provides
access to module documentation defined in docstrings  without requiring
that the code being examined be loaded into a running interpreter via
`import'.  This can be very useful for performing analyses of untrusted
code.

Generally, the example will demonstrate how the parse tree may be
traversed to distill interesting information.  Two functions and a set
of classes are developed which provide programmatic access to high
level function and class definitions provided by a module.  The classes
extract information from the parse tree and provide access to the
information at a useful semantic level, one function provides a simple
low-level pattern matching capability, and the other function defines a
high-level interface to the classes by handling file operations on
behalf of the caller.  All source files mentioned here which are not
part of the Python installation are located in the `Demo/parser/'
directory of the distribution.

The dynamic nature of Python allows the programmer a great deal of
flexibility, but most modules need only a limited measure of this when
defining classes, functions, and methods.  In this example, the only
definitions that will be considered are those which are defined in the
top level of their context, e.g., a function defined by a `def'
statement at column zero of a module, but not a function defined within
a branch of an `if' ... `else' construct, though there are some good
reasons for doing so in some situations.  Nesting of definitions will
be handled by the code developed in the example.

To construct the upper-level extraction methods, we need to know what
the parse tree structure looks like and how much of it we actually need
to be concerned about.  Python uses a moderately deep parse tree so
there are a large number of intermediate nodes.  It is important to
read and understand the formal grammar used by Python.  This is
specified in the file `Grammar/Grammar' in the distribution.  Consider
the simplest case of interest when searching for docstrings: a module
consisting of a docstring and nothing else.  (See file `docstring.py'.)

     """Some documentation.
     """

Using the interpreter to take a look at the parse tree, we find a
bewildering mass of numbers and parentheses, with the documentation
buried deep in nested tuples.

     >>> import parser
     >>> import pprint
     >>> ast = parser.suite(open('docstring.py').read())
     >>> tup = ast.totuple()
     >>> pprint.pprint(tup)
     (257,
      (264,
       (265,
        (266,
         (267,
          (307,
           (287,
            (288,
             (289,
              (290,
               (292,
                (293,
                 (294,
                  (295,
                   (296,
                    (297,
                     (298,
                      (299,
                       (300, (3, '"""Some documentation.\n"""'))))))))))))))))),
        (4, ''))),
      (4, ''),
      (0, ''))

The numbers at the first element of each node in the tree are the node
types; they map directly to terminal and non-terminal symbols in the
grammar.  Unfortunately, they are represented as integers in the
internal representation, and the Python structures generated do not
change that.  However, the `symbol' and `token' modules provide
symbolic names for the node types and dictionaries which map from the
integers to the symbolic names for the node types.

In the output presented above, the outermost tuple contains four
elements: the integer `257' and three additional tuples.  Node type
`257' has the symbolic name `file_input'.  Each of these inner tuples
contains an integer as the first element; these integers, `264', `4',
and `0', represent the node types `stmt', `NEWLINE', and `ENDMARKER',
respectively.  Note that these values may change depending on the
version of Python you are using; consult `symbol.py' and `token.py' for
details of the mapping.  It should be fairly clear that the outermost
node is related primarily to the input source rather than the contents
of the file, and may be disregarded for the moment.  The `stmt' node is
much more interesting.  In particular, all docstrings are found in
subtrees which are formed exactly as this node is formed, with the only
difference being the string itself.  The association between the
docstring in a similar tree and the defined entity (class, function, or
module) which it describes is given by the position of the docstring
subtree within the tree defining the described structure.

By replacing the actual docstring with something to signify a variable
component of the tree, we allow a simple pattern matching approach to
check any given subtree for equivalence to the general pattern for
docstrings.  Since the example demonstrates information extraction, we
can safely require that the tree be in tuple form rather than list
form, allowing a simple variable representation to be
`['variable_name']'.  A simple recursive function can implement the
pattern matching, returning a Boolean and a dictionary of variable name
to value mappings.  (See file `example.py'.)

     from types import ListType, TupleType
     
     def match(pattern, data, vars=None):
         if vars is None:
             vars = {}
         if type(pattern) is ListType:
             vars[pattern[0]] = data
             return 1, vars
         if type(pattern) is not TupleType:
             return (pattern == data), vars
         if len(data) != len(pattern):
             return 0, vars
         for pattern, data in map(None, pattern, data):
             same, vars = match(pattern, data, vars)
             if not same:
                 break
         return same, vars

Using this simple representation for syntactic variables and the
symbolic node types, the pattern for the candidate docstring subtrees
becomes fairly readable.  (See file `example.py'.)

     import symbol
     import token
     
     DOCSTRING_STMT_PATTERN = (
         symbol.stmt,
         (symbol.simple_stmt,
          (symbol.small_stmt,
           (symbol.expr_stmt,
            (symbol.testlist,
             (symbol.test,
              (symbol.and_test,
               (symbol.not_test,
                (symbol.comparison,
                 (symbol.expr,
                  (symbol.xor_expr,
                   (symbol.and_expr,
                    (symbol.shift_expr,
                     (symbol.arith_expr,
                      (symbol.term,
                       (symbol.factor,
                        (symbol.power,
                         (symbol.atom,
                          (token.STRING, ['docstring'])
                          )))))))))))))))),
          (token.NEWLINE, '')
          ))

Using the `match()' function with this pattern, extracting the module
docstring from the parse tree created previously is easy:

     >>> found, vars = match(DOCSTRING_STMT_PATTERN, tup[1])
     >>> found
     1
     >>> vars
     {'docstring': '"""Some documentation.\n"""'}

Once specific data can be extracted from a location where it is
expected, the question of where information can be expected needs to be
answered.  When dealing with docstrings, the answer is fairly simple:
the docstring is the first `stmt' node in a code block (`file_input' or
`suite' node types).  A module consists of a single `file_input' node,
and class and function definitions each contain exactly one `suite'
node.  Classes and functions are readily identified as subtrees of code
block nodes which start with `(stmt, (compound_stmt, (classdef, ...' or
`(stmt, (compound_stmt, (funcdef, ...'.  Note that these subtrees
cannot be matched by `match()' since it does not support multiple
sibling nodes to match without regard to number.  A more elaborate
matching function could be used to overcome this limitation, but this
is sufficient for the example.

Given the ability to determine whether a statement might be a docstring
and extract the actual string from the statement, some work needs to be
performed to walk the parse tree for an entire module and extract
information about the names defined in each context of the module and
associate any docstrings with the names.  The code to perform this work
is not complicated, but bears some explanation.

The public interface to the classes is straightforward and should
probably be somewhat more flexible.  Each "major" block of the module
is described by an object providing several methods for inquiry and a
constructor which accepts at least the subtree of the complete parse
tree which it represents.  The `ModuleInfo' constructor accepts an
optional NAME parameter since it cannot otherwise determine the name of
the module.

The public classes include `ClassInfo', `FunctionInfo', and
`ModuleInfo'.  All objects provide the methods `get_name()',
`get_docstring()', `get_class_names()', and `get_class_info()'.  The
`ClassInfo' objects support `get_method_names()' and
`get_method_info()' while the other classes provide
`get_function_names()' and `get_function_info()'.

Within each of the forms of code block that the public classes
represent, most of the required information is in the same form and is
accessed in the same way, with classes having the distinction that
functions defined at the top level are referred to as "methods."  Since
the difference in nomenclature reflects a real semantic distinction
from functions defined outside of a class, the implementation needs to
maintain the distinction.  Hence, most of the functionality of the
public classes can be implemented in a common base class,
`SuiteInfoBase', with the accessors for function and method information
provided elsewhere.  Note that there is only one class which represents
function and method information; this parallels the use of the `def'
statement to define both types of elements.

Most of the accessor functions are declared in `SuiteInfoBase' and do
not need to be overridden by subclasses.  More importantly, the
extraction of most information from a parse tree is handled through a
method called by the `SuiteInfoBase' constructor.  The example code for
most of the classes is clear when read alongside the formal grammar,
but the method which recursively creates new information objects
requires further examination.  Here is the relevant part of the
`SuiteInfoBase' definition from `example.py':

     class SuiteInfoBase:
         _docstring = ''
         _name = ''
     
         def __init__(self, tree = None):
             self._class_info = {}
             self._function_info = {}
             if tree:
                 self._extract_info(tree)
     
         def _extract_info(self, tree):
             # extract docstring
             if len(tree) == 2:
                 found, vars = match(DOCSTRING_STMT_PATTERN[1], tree[1])
             else:
                 found, vars = match(DOCSTRING_STMT_PATTERN, tree[3])
             if found:
                 self._docstring = eval(vars['docstring'])
             # discover inner definitions
             for node in tree[1:]:
                 found, vars = match(COMPOUND_STMT_PATTERN, node)
                 if found:
                     cstmt = vars['compound']
                     if cstmt[0] == symbol.funcdef:
                         name = cstmt[2][1]
                         self._function_info[name] = FunctionInfo(cstmt)
                     elif cstmt[0] == symbol.classdef:
                         name = cstmt[2][1]
                         self._class_info[name] = ClassInfo(cstmt)

After initializing some internal state, the constructor calls the
`_extract_info()' method.  This method performs the bulk of the
information extraction which takes place in the entire example.  The
extraction has two distinct phases: the location of the docstring for
the parse tree passed in, and the discovery of additional definitions
within the code block represented by the parse tree.

The initial `if' test determines whether the nested suite is of the
"short form" or the "long form."  The short form is used when the code
block is on the same line as the definition of the code block, as in

     def square(x): "Square an argument."; return x ** 2

while the long form uses an indented block and allows nested
definitions:

     def make_power(exp):
         "Make a function that raises an argument to the exponent `exp'."
         def raiser(x, y=exp):
             return x ** y
         return raiser

When the short form is used, the code block may contain a docstring as
the first, and possibly only, `small_stmt' element.  The extraction of
such a docstring is slightly different and requires only a portion of
the complete pattern used in the more common case.  As implemented, the
docstring will only be found if there is only one `small_stmt' node in
the `simple_stmt' node.  Since most functions and methods which use the
short form do not provide a docstring, this may be considered
sufficient.  The extraction of the docstring proceeds using the
`match()' function as described above, and the value of the docstring
is stored as an attribute of the `SuiteInfoBase' object.

After docstring extraction, a simple definition discovery algorithm
operates on the `stmt' nodes of the `suite' node.  The special case of
the short form is not tested; since there are no `stmt' nodes in the
short form, the algorithm will silently skip the single `simple_stmt'
node and correctly not discover any nested definitions.

Each statement in the code block is categorized as a class definition,
function or method definition, or something else.  For the definition
statements, the name of the element defined is extracted and a
representation object appropriate to the definition is created with the
defining subtree passed as an argument to the constructor.  The
representation objects are stored in instance variables and may be
retrieved by name using the appropriate accessor methods.

The public classes provide any accessors required which are more
specific than those provided by the `SuiteInfoBase' class, but the real
extraction algorithm remains common to all forms of code blocks.  A
high-level function can be used to extract the complete set of
information from a source file.  (See file `example.py'.)

     def get_docs(fileName):
         import os
         import parser
     
         source = open(fileName).read()
         basename = os.path.basename(os.path.splitext(fileName)[0])
         ast = parser.suite(source)
         return ModuleInfo(ast.totuple(), basename)

This provides an easy-to-use interface to the documentation of a
module.  If information is required which is not extracted by the code
of this example, the code may be extended at clearly defined points to
provide additional capabilities.


File: python-lib.info,  Node: symbol,  Next: token,  Prev: parser,  Up: Python Language Services

Constants used with Python parse trees
======================================

Constants representing internal nodes of the parse tree.

This module provides constants which represent the numeric values of
internal nodes of the parse tree.  Unlike most Python constants, these
use lower-case names.  Refer to the file `Grammar/Grammar' in the
Python distribution for the definitions of the names in the context of
the language grammar.  The specific numeric values which the names map
to may change between Python versions.

This module also provides one additional data object:

`sym_name'
     Dictionary mapping the numeric values of the constants defined in
     this module back to name strings, allowing more human-readable
     representation of parse trees to be generated.

See also:
     *Note parser:: The second example for the `parser' module shows
     how to use the `symbol' module.


File: python-lib.info,  Node: token,  Next: keyword,  Prev: symbol,  Up: Python Language Services

Constants used with Python parse trees
======================================

Constants representing terminal nodes of the parse tree.

This module provides constants which represent the numeric values of
leaf nodes of the parse tree (terminal tokens).  Refer to the file
`Grammar/Grammar' in the Python distribution for the definitions of the
names in the context of the language grammar.  The specific numeric
values which the names map to may change between Python versions.

This module also provides one data object and some functions.  The
functions mirror definitions in the Python C header files.

`tok_name'
     Dictionary mapping the numeric values of the constants defined in
     this module back to name strings, allowing more human-readable
     representation of parse trees to be generated.

`ISTERMINAL(x)'
     Return true for terminal token values.

`ISNONTERMINAL(x)'
     Return true for non-terminal token values.

`ISEOF(x)'
     Return true if X is the marker indicating the end of input.

See also:
     *Note parser:: The second example for the `parser' module shows
     how to use the `symbol' module.


File: python-lib.info,  Node: keyword,  Next: tokenize,  Prev: token,  Up: Python Language Services

Testing for Python keywords
===========================

Test whether a string is a keyword in Python.

This module allows a Python program to determine if a string is a
keyword.

`iskeyword(s)'
     Return true if S is a Python keyword.

`kwlist'
     Sequence containing all the keywords defined for the interpreter.
     If any keywords are defined to only be active when particular
     `__future__' statements are in effect, these will be included as
     well.


File: python-lib.info,  Node: tokenize,  Next: tabnanny,  Prev: keyword,  Up: Python Language Services

Tokenizer for Python source
===========================

Lexical scanner for Python source code.

The `tokenize' module provides a lexical scanner for Python source
code, implemented in Python.  The scanner in this module returns
comments as tokens as well, making it useful for implementing
"pretty-printers," including colorizers for on-screen displays.

The primary entry point is a generator:

`generate_tokens(readline)'
     The `generate_tokens()' generator requires one argment, READLINE,
     which must be a callable object which provides the same interface
     as the `readline()' method of built-in file objects (see
     section~*Note File Objects::).  Each call to the function should
     return one line of input as a string.

     The generator produces 5-tuples with these members: the token type;
     the token string; a 2-tuple `(SROW, SCOL)' of ints specifying the
     row and column where the token begins in the source; a 2-tuple
     `(EROW, ECOL)' of ints specifying the row and column where the
     token ends in the source; and the line on which the token was
     found.  The line passed is the _logical_ line; continuation lines
     are included.  _Added in Python version 2.2_

An older entry point is retained for backward compatibility:

`tokenize(readline[, tokeneater])'
     The `tokenize()' function accepts two parameters: one representing
     the input stream, and one providing an output mechanism for
     `tokenize()'.

     The first parameter, READLINE, must be a callable object which
     provides the same interface as the `readline()' method of built-in
     file objects (see section~*Note File Objects::).  Each call to the
     function should return one line of input as a string.

     The second parameter, TOKENEATER, must also be a callable object.
     It is called once for each token, with five arguments,
     corresponding to the tuples generated by `generate_tokens()'.

All constants from the `token' module are also exported from
`tokenize', as are two additional token type values that might be
passed to the TOKENEATER function by `tokenize()':

`COMMENT'
     Token value used to indicate a comment.

`NL'
     Token value used to indicate a non-terminating newline.  The
     NEWLINE token indicates the end of a logical line of Python code;
     NL tokens are generated when a logical line of code is continued
     over multiple physical lines.


File: python-lib.info,  Node: tabnanny,  Next: pyclbr,  Prev: tokenize,  Up: Python Language Services

Detection of ambiguous indentation
==================================

Tool for detecting white space related problems in Python source files
in a directory tree.

For the time being this module is intended to be called as a script.
However it is possible to import it into an IDE and use the function
`check()' described below.

_The API provided by this module is likely to change  in future
releases; such changes may not be backward compatible._

`check(file_or_dir)'
     If FILE_OR_DIR is a directory and not a symbolic link, then
     recursively descend the directory tree named by FILE_OR_DIR,
     checking all `.py' files along the way.  If FILE_OR_DIR is an
     ordinary Python source file, it is checked for whitespace related
     problems.  The diagnostic messages are written to standard output
     using the print statement.

`verbose'
     Flag indicating whether to print verbose messages.  This is
     incremented by the `-v' option if called as a script.

`filename_only'
     Flag indicating whether to print only the filenames of files
     containing whitespace related problems.  This is set to true by the
     `-q' option if called as a script.

`NannyNag'
     Raised by `tokeneater()' if detecting an ambiguous indent.
     Captured and handled in `check()'.

`tokeneater(type, token, start, end, line)'
     This function is used by `check()' as a callback parameter to the
     function `tokenize.tokenize()'.

See also:
     *Note tokenize:: Lexical scanner for Python source code.


File: python-lib.info,  Node: pyclbr,  Next: py_compile,  Prev: tabnanny,  Up: Python Language Services

Python class browser support
============================

Supports information extraction for a Python class browser.

The `pyclbr' can be used to determine some limited information about
the classes and methods defined in a module.  The information provided
is sufficient to implement a traditional three-pane class browser.  The
information is extracted from the source code rather than from an
imported module, so this module is safe to use with untrusted source
code.  This restriction makes it impossible to use this module with
modules not implemented in Python, including many standard and optional
extension modules.

`readmodule(module[, path])'
     Read a module and return a dictionary mapping class names to class
     descriptor objects.  The parameter MODULE should be the name of a
     module as a string; it may be the name of a module within a
     package.  The PATH parameter should be a sequence, and is used to
     augment the value of `sys.path', which is used to locate module
     source code.

* Menu:

* Class Descriptor Objects::


File: python-lib.info,  Node: Class Descriptor Objects,  Prev: pyclbr,  Up: pyclbr

Class Descriptor Objects
------------------------

The class descriptor objects used as values in the dictionary returned
by `readmodule()' provide the following data members:

`module'
     The name of the module defining the class described by the class
     descriptor.

`name'
     The name of the class.

`super'
     A list of class descriptors which describe the immediate base
     classes of the class being described.  Classes which are named as
     superclasses but which are not discoverable by `readmodule()' are
     listed as a string with the class name instead of class
     descriptors.

`methods'
     A dictionary mapping method names to line numbers.

`file'
     Name of the file containing the class statement defining the class.

`lineno'
     The line number of the class statement within the file named by
     `file'.


File: python-lib.info,  Node: py_compile,  Next: compileall,  Prev: pyclbr,  Up: Python Language Services

Compile Python source files
===========================

Compile Python source files to byte-code files.

The `py_compile' module provides a function to generate a byte-code
file from a source file, and another function used when the module
source file is invoked as a script.

Though not often needed, this function can be useful when installing
modules for shared use, especially if some of the users may not have
permission to write the byte-code cache files in the directory
containing the source code.

`PyCompileError'
     Exception raised when an error occurs while attempting to compile
     the file.

`compile(file[, cfile[, dfile[, doraise]]])'
     Compile a source file to byte-code and write out the byte-code
     cache file.  The source code is loaded from the file name FILE.
     The byte-code is written to CFILE, which defaults to FILE `+'
     `'c'' (`'o'' if optimization is enabled in the current
     interpreter).  If DFILE is specified, it is used as the name of
     the source file in error messages instead of FILE.  If DORAISE =
     True, a PyCompileError is raised when an error is encountered
     while compiling FILE. If DORAISE = False (the default), an error
     string is written to sys.stderr, but no exception is raised.

`main([args])'
     Compile several source files.  The files named in ARGS (or on the
     command line, if ARGS is not specified) are compiled and the
     resulting bytecode is cached in the normal manner.  This function
     does not search a directory structure to locate source files; it
     only compiles files named explicitly.

When this module is run as a script, the `main()' is used to compile
all the files named on the command line.

See also:
     *Note compileall:: Utilities to compile all Python source files in
     a directory tree.


File: python-lib.info,  Node: compileall,  Next: dis,  Prev: py_compile,  Up: Python Language Services

Byte-compile Python libraries
=============================

Tools for byte-compiling all Python source files in a directory tree.

This module provides some utility functions to support installing
Python libraries.  These functions compile Python source files in a
directory tree, allowing users without permission to write to the
libraries to take advantage of cached byte-code files.

The source file for this module may also be used as a script to compile
Python sources in directories named on the command line or in
`sys.path'.

`compile_dir(dir[, maxlevels[, ddir[, force[,  rx[, quiet]]]]])'
     Recursively descend the directory tree named by DIR, compiling all
     `.py' files along the way.  The MAXLEVELS parameter is used to
     limit the depth of the recursion; it defaults to `10'.  If DDIR is
     given, it is used as the base path from which the filenames used
     in error messages will be generated.  If FORCE is true, modules
     are re-compiled even if the timestamps are up to date.

     If RX is given, it specifies a regular expression of file names to
     exclude from the search; that expression is searched for in the
     full path.

     If QUIET is true, nothing is printed to the standard output in
     normal operation.

`compile_path([skip_curdir[, maxlevels[, force]]])'
     Byte-compile all the `.py' files found along `sys.path'.  If
     SKIP_CURDIR is true (the default), the current directory is not
     included in the search.  The MAXLEVELS and FORCE parameters
     default to `0' and are passed to the `compile_dir()' function.

See also:
     *Note py_compile:: Byte-compile a single source file.


File: python-lib.info,  Node: dis,  Next: distutils,  Prev: compileall,  Up: Python Language Services

Disassembler for Python byte code
=================================

Disassembler for Python byte code.

The `dis' module supports the analysis of Python byte code by
disassembling it.  Since there is no Python assembler, this module
defines the Python assembly language.  The Python byte code which this
module takes as an input is defined in the file `Include/opcode.h' and
used by the compiler and the interpreter.

Example: Given the function `myfunc':

     def myfunc(alist):
         return len(alist)

the following command can be used to get the disassembly of `myfunc()':

     >>> dis.dis(myfunc)
       2           0 LOAD_GLOBAL              0 (len)
                   3 LOAD_FAST                0 (alist)
                   6 CALL_FUNCTION            1
                   9 RETURN_VALUE
                  10 LOAD_CONST               0 (None)
                  13 RETURN_VALUE

(The "2" is a line number).

The `dis' module defines the following functions and constants:

`dis([bytesource])'
     Disassemble the BYTESOURCE object. BYTESOURCE can denote either a
     module, a class, a method, a function, or a code object.  For a
     module, it disassembles all functions.  For a class, it
     disassembles all methods.  For a single code sequence, it prints
     one line per byte code instruction.  If no object is provided, it
     disassembles the last traceback.

`distb([tb])'
     Disassembles the top-of-stack function of a traceback, using the
     last traceback if none was passed.  The instruction causing the
     exception is indicated.

`disassemble(code[, lasti])'
     Disassembles a code object, indicating the last instruction if
     LASTI was provided.  The output is divided in the following
     columns:

       1. the line number, for the first instruction of each line

       2. the current instruction, indicated as `-->',

       3. a labelled instruction, indicated with `>`>'',

       4. the address of the instruction,

       5. the operation code name,

       6. operation parameters, and

       7. interpretation of the parameters in parentheses.

     The parameter interpretation recognizes local and global variable
     names, constant values, branch targets, and compare operators.

`disco(code[, lasti])'
     A synonym for disassemble.  It is more convenient to type, and kept
     for compatibility with earlier Python releases.

`opname'
     Sequence of operation names, indexable using the byte code.

`cmp_op'
     Sequence of all compare operation names.

`hasconst'
     Sequence of byte codes that have a constant parameter.

`hasfree'
     Sequence of byte codes that access a free variable.

`hasname'
     Sequence of byte codes that access an attribute by name.

`hasjrel'
     Sequence of byte codes that have a relative jump target.

`hasjabs'
     Sequence of byte codes that have an absolute jump target.

`haslocal'
     Sequence of byte codes that access a local variable.

`hascompare'
     Sequence of byte codes of Boolean operations.

* Menu:

* Python Byte Code Instructions::


File: python-lib.info,  Node: Python Byte Code Instructions,  Prev: dis,  Up: dis

Python Byte Code Instructions
-----------------------------

The Python compiler currently generates the following byte code
instructions.

`STOP_CODE'
     Indicates end-of-code to the compiler, not used by the interpreter.

`POP_TOP'
     Removes the top-of-stack (TOS) item.

`ROT_TWO'
     Swaps the two top-most stack items.

`ROT_THREE'
     Lifts second and third stack item one position up, moves top down
     to position three.

`ROT_FOUR'
     Lifts second, third and forth stack item one position up, moves
     top down to position four.

`DUP_TOP'
     Duplicates the reference on top of the stack.

Unary Operations take the top of the stack, apply the operation, and
push the result back on the stack.

`UNARY_POSITIVE'
     Implements `TOS = +TOS'.

`UNARY_NEGATIVE'
     Implements `TOS = -TOS'.

`UNARY_NOT'
     Implements `TOS = not TOS'.

`UNARY_CONVERT'
     Implements `TOS = `TOS`'.

`UNARY_INVERT'
     Implements `TOS = ~{}TOS'.

`GET_ITER'
     Implements `TOS = iter(TOS)'.

Binary operations remove the top of the stack (TOS) and the second
top-most stack item (TOS1) from the stack.  They perform the operation,
and put the result back on the stack.

`BINARY_POWER'
     Implements `TOS = TOS1 ** TOS'.

`BINARY_MULTIPLY'
     Implements `TOS = TOS1 * TOS'.

`BINARY_DIVIDE'
     Implements `TOS = TOS1 / TOS' when `from __future__ import
     division' is not in effect.

`BINARY_FLOOR_DIVIDE'
     Implements `TOS = TOS1 // TOS'.

`BINARY_TRUE_DIVIDE'
     Implements `TOS = TOS1 / TOS' when `from __future__ import
     division' is in effect.

`BINARY_MODULO'
     Implements `TOS = TOS1 %{} TOS'.

`BINARY_ADD'
     Implements `TOS = TOS1 + TOS'.

`BINARY_SUBTRACT'
     Implements `TOS = TOS1 - TOS'.

`BINARY_SUBSCR'
     Implements `TOS = TOS1[TOS]'.

`BINARY_LSHIFT'
     Implements `TOS = TOS1 <`'< TOS'.

`BINARY_RSHIFT'
     Implements `TOS = TOS1 >`'> TOS'.

`BINARY_AND'
     Implements `TOS = TOS1 & TOS'.

`BINARY_XOR'
     Implements `TOS = TOS1 ^ TOS'.

`BINARY_OR'
     Implements `TOS = TOS1 | TOS'.

In-place operations are like binary operations, in that they remove TOS
and TOS1, and push the result back on the stack, but the operation is
done in-place when TOS1 supports it, and the resulting TOS may be (but
does not have to be) the original TOS1.

`INPLACE_POWER'
     Implements in-place `TOS = TOS1 ** TOS'.

`INPLACE_MULTIPLY'
     Implements in-place `TOS = TOS1 * TOS'.

`INPLACE_DIVIDE'
     Implements in-place `TOS = TOS1 / TOS' when `from __future__
     import division' is not in effect.

`INPLACE_FLOOR_DIVIDE'
     Implements in-place `TOS = TOS1 // TOS'.

`INPLACE_TRUE_DIVIDE'
     Implements in-place `TOS = TOS1 / TOS' when `from __future__
     import division' is in effect.

`INPLACE_MODULO'
     Implements in-place `TOS = TOS1 %{} TOS'.

`INPLACE_ADD'
     Implements in-place `TOS = TOS1 + TOS'.

`INPLACE_SUBTRACT'
     Implements in-place `TOS = TOS1 - TOS'.

`INPLACE_LSHIFT'
     Implements in-place `TOS = TOS1 <`'< TOS'.

`INPLACE_RSHIFT'
     Implements in-place `TOS = TOS1 >`'> TOS'.

`INPLACE_AND'
     Implements in-place `TOS = TOS1 & TOS'.

`INPLACE_XOR'
     Implements in-place `TOS = TOS1 ^ TOS'.

`INPLACE_OR'
     Implements in-place `TOS = TOS1 | TOS'.

The slice opcodes take up to three parameters.

`SLICE+0'
     Implements `TOS = TOS[:]'.

`SLICE+1'
     Implements `TOS = TOS1[TOS:]'.

`SLICE+2'
     Implements `TOS = TOS1[:TOS]'.

`SLICE+3'
     Implements `TOS = TOS2[TOS1:TOS]'.

Slice assignment needs even an additional parameter.  As any statement,
they put nothing on the stack.

`STORE_SLICE+0'
     Implements `TOS[:] = TOS1'.

`STORE_SLICE+1'
     Implements `TOS1[TOS:] = TOS2'.

`STORE_SLICE+2'
     Implements `TOS1[:TOS] = TOS2'.

`STORE_SLICE+3'
     Implements `TOS2[TOS1:TOS] = TOS3'.

`DELETE_SLICE+0'
     Implements `del TOS[:]'.

`DELETE_SLICE+1'
     Implements `del TOS1[TOS:]'.

`DELETE_SLICE+2'
     Implements `del TOS1[:TOS]'.

`DELETE_SLICE+3'
     Implements `del TOS2[TOS1:TOS]'.

`STORE_SUBSCR'
     Implements `TOS1[TOS] = TOS2'.

`DELETE_SUBSCR'
     Implements `del TOS1[TOS]'.

Miscellaneous opcodes.

`PRINT_EXPR'
     Implements the expression statement for the interactive mode.  TOS
     is removed from the stack and printed.  In non-interactive mode, an
     expression statement is terminated with `POP_STACK'.

`PRINT_ITEM'
     Prints TOS to the file-like object bound to `sys.stdout'.  There
     is one such instruction for each item in the `print' statement.

`PRINT_ITEM_TO'
     Like `PRINT_ITEM', but prints the item second from TOS to the
     file-like object at TOS.  This is used by the extended print
     statement.

`PRINT_NEWLINE'
     Prints a new line on `sys.stdout'.  This is generated as the last
     operation of a `print' statement, unless the statement ends with a
     comma.

`PRINT_NEWLINE_TO'
     Like `PRINT_NEWLINE', but prints the new line on the file-like
     object on the TOS.  This is used by the extended print statement.

`BREAK_LOOP'
     Terminates a loop due to a `break' statement.

`CONTINUE_LOOP target'
     Continues a loop due to a `continue' statement.  TARGET is the
     address to jump to (which should be a `FOR_ITER' instruction).

`LOAD_LOCALS'
     Pushes a reference to the locals of the current scope on the stack.
     This is used in the code for a class definition: After the class
     body is evaluated, the locals are passed to the class definition.

`RETURN_VALUE'
     Returns with TOS to the caller of the function.

`YIELD_VALUE'
     Pops `TOS' and yields it from a generator.

`IMPORT_STAR'
     Loads all symbols not starting with `_' directly from the module
     TOS to the local namespace. The module is popped after loading all
     names.  This opcode implements `from module import *'.

`EXEC_STMT'
     Implements `exec TOS2,TOS1,TOS'.  The compiler fills missing
     optional parameters with `None'.

`POP_BLOCK'
     Removes one block from the block stack.  Per frame, there is a
     stack of blocks, denoting nested loops, try statements, and such.

`END_FINALLY'
     Terminates a `finally' clause.  The interpreter recalls whether
     the exception has to be re-raised, or whether the function
     returns, and continues with the outer-next block.

`BUILD_CLASS'
     Creates a new class object.  TOS is the methods dictionary, TOS1
     the tuple of the names of the base classes, and TOS2 the class
     name.

All of the following opcodes expect arguments.  An argument is two
bytes, with the more significant byte last.

`STORE_NAME namei'
     Implements `name = TOS'. NAMEI is the index of NAME in the
     attribute `co_names' of the code object.  The compiler tries to
     use `STORE_LOCAL' or `STORE_GLOBAL' if possible.

`DELETE_NAME namei'
     Implements `del name', where NAMEI is the index into `co_names'
     attribute of the code object.

`UNPACK_SEQUENCE count'
     Unpacks TOS into COUNT individual values, which are put onto the
     stack right-to-left.

`DUP_TOPX count'
     Duplicate COUNT items, keeping them in the same order. Due to
     implementation limits, COUNT should be between 1 and 5 inclusive.

`STORE_ATTR namei'
     Implements `TOS.name = TOS1', where NAMEI is the index of name in
     `co_names'.

`DELETE_ATTR namei'
     Implements `del TOS.name', using NAMEI as index into `co_names'.

`STORE_GLOBAL namei'
     Works as `STORE_NAME', but stores the name as a global.

`DELETE_GLOBAL namei'
     Works as `DELETE_NAME', but deletes a global name.

`LOAD_CONST consti'
     Pushes `co_consts[CONSTI]' onto the stack.

`LOAD_NAME namei'
     Pushes the value associated with `co_names[NAMEI]' onto the stack.

`BUILD_TUPLE count'
     Creates a tuple consuming COUNT items from the stack, and pushes
     the resulting tuple onto the stack.

`BUILD_LIST count'
     Works as `BUILD_TUPLE', but creates a list.

`BUILD_MAP zero'
     Pushes a new empty dictionary object onto the stack.  The argument
     is ignored and set to zero by the compiler.

`LOAD_ATTR namei'
     Replaces TOS with `getattr(TOS, co_names[NAMEI])'.

`COMPARE_OP opname'
     Performs a Boolean operation.  The operation name can be found in
     `cmp_op[OPNAME]'.

`IMPORT_NAME namei'
     Imports the module `co_names[NAMEI]'.  The module object is pushed
     onto the stack.  The current namespace is not affected: for a
     proper import statement, a subsequent `STORE_FAST' instruction
     modifies the namespace.

`IMPORT_FROM namei'
     Loads the attribute `co_names[NAMEI]' from the module found in
     TOS. The resulting object is pushed onto the stack, to be
     subsequently stored by a `STORE_FAST' instruction.

`JUMP_FORWARD delta'
     Increments byte code counter by DELTA.

`JUMP_IF_TRUE delta'
     If TOS is true, increment the byte code counter by DELTA.  TOS is
     left on the stack.

`JUMP_IF_FALSE delta'
     If TOS is false, increment the byte code counter by DELTA.  TOS is
     not changed.

`JUMP_ABSOLUTE target'
     Set byte code counter to TARGET.

`FOR_ITER delta'
     `TOS' is an iterator.  Call its `next()' method.  If this yields a
     new value, push it on the stack (leaving the iterator below it).
     If the iterator indicates it is exhausted  `TOS' is popped, and
     the byte code counter is incremented by DELTA.

`LOAD_GLOBAL namei'
     Loads the global named `co_names[NAMEI]' onto the stack.

`SETUP_LOOP delta'
     Pushes a block for a loop onto the block stack.  The block spans
     from the current instruction with a size of DELTA bytes.

`SETUP_EXCEPT delta'
     Pushes a try block from a try-except clause onto the block stack.
     DELTA points to the first except block.

`SETUP_FINALLY delta'
     Pushes a try block from a try-except clause onto the block stack.
     DELTA points to the finally block.

`LOAD_FAST var_num'
     Pushes a reference to the local `co_varnames[VAR_NUM]' onto the
     stack.

`STORE_FAST var_num'
     Stores TOS into the local `co_varnames[VAR_NUM]'.

`DELETE_FAST var_num'
     Deletes local `co_varnames[VAR_NUM]'.

`LOAD_CLOSURE i'
     Pushes a reference to the cell contained in slot I of the cell and
     free variable storage.  The name of the variable is
     `co_cellvars[I]' if I is less than the length of CO_CELLVARS.
     Otherwise it is `co_freevars[I - len(co_cellvars)]'.

`LOAD_DEREF i'
     Loads the cell contained in slot I of the cell and free variable
     storage.  Pushes a reference to the object the cell contains on the
     stack.

`STORE_DEREF i'
     Stores TOS into the cell contained in slot I of the cell and free
     variable storage.

`SET_LINENO lineno'
     This opcode is obsolete.

`RAISE_VARARGS argc'
     Raises an exception. ARGC indicates the number of parameters to
     the raise statement, ranging from 0 to 3.  The handler will find
     the traceback as TOS2, the parameter as TOS1, and the exception as
     TOS.

`CALL_FUNCTION argc'
     Calls a function.  The low byte of ARGC indicates the number of
     positional parameters, the high byte the number of keyword
     parameters.  On the stack, the opcode finds the keyword parameters
     first.  For each keyword argument, the value is on top of the key.
     Below the keyword parameters, the positional parameters are on
     the stack, with the right-most parameter on top.  Below the
     parameters, the function object to call is on the stack.

`MAKE_FUNCTION argc'
     Pushes a new function object on the stack.  TOS is the code
     associated with the function.  The function object is defined to
     have ARGC default parameters, which are found below TOS.

`MAKE_CLOSURE argc'
     Creates a new function object, sets its FUNC_CLOSURE slot, and
     pushes it on the stack.  TOS is the code associated with the
     function.  If the code object has N free variables, the next N
     items on the stack are the cells for these variables.  The
     function also has ARGC default parameters, where are found before
     the cells.

`BUILD_SLICE argc'
     Pushes a slice object on the stack.  ARGC must be 2 or 3.  If it
     is 2, `slice(TOS1, TOS)' is pushed; if it is 3, `slice(TOS2, TOS1,
     TOS)' is pushed.  See the `slice()'  built-in function for more
     information.

`EXTENDED_ARG ext'
     Prefixes any opcode which has an argument too big to fit into the
     default two bytes.  EXT holds two additional bytes which, taken
     together with the subsequent opcode's argument, comprise a
     four-byte argument, EXT being the two most-significant bytes.

`CALL_FUNCTION_VAR argc'
     Calls a function. ARGC is interpreted as in `CALL_FUNCTION'.  The
     top element on the stack contains the variable argument list,
     followed by keyword and positional arguments.

`CALL_FUNCTION_KW argc'
     Calls a function. ARGC is interpreted as in `CALL_FUNCTION'.  The
     top element on the stack contains the keyword arguments dictionary,
     followed by explicit keyword and positional arguments.

`CALL_FUNCTION_VAR_KW argc'
     Calls a function. ARGC is interpreted as in `CALL_FUNCTION'.  The
     top element on the stack contains the keyword arguments
     dictionary, followed by the variable-arguments tuple, followed by
     explicit keyword and positional arguments.


File: python-lib.info,  Node: distutils,  Prev: dis,  Up: Python Language Services

Building and installing Python modules
======================================

Support for building and installing Python modules into an existing
Python installation.

The `distutils' package provides support for building and installing
additional modules into a Python installation.  The new modules may be
either 100%{}-pure Python, or may be extension modules written in C, or
may be collections of Python packages which include modules coded in
both Python and C.

This package is discussed in two separate documents which are included
in the Python documentation package.  To learn about distributing new
modules using the `distutils' facilities, read .  To learn about
installing Python modules, whether or not the author made use of the
`distutils' package, read .

See also:
     `Distributing Python Modules'{The manual for developers and
     packagers of Python modules.  This describes how to prepare
     `distutils'-based packages so that they may be easily installed
     into an existing Python installaion.}

     `Installing Python Modules'{An "administrators" manual which
     includes information on installing modules into an existing Python
     installation. You do not need to be a Python programmer to read
     this manual.}


File: python-lib.info,  Node: Python compiler package,  Next: SGI IRIX Specific Services,  Prev: Python Language Services,  Up: Top

Python compiler package
***********************

The Python compiler package is a tool for analyzing Python source code
and generating Python bytecode.  The compiler contains libraries to
generate an abstract syntax tree from Python source code and to
generate Python bytecode from the tree.

The `compiler' package is a Python source to bytecode translator
written in Python.  It uses the built-in parser and standard `parser'
module to generated a concrete syntax tree.  This tree is used to
generate an abstract syntax tree (AST) and then Python bytecode.

The full functionality of the package duplicates the builtin compiler
provided with the Python interpreter.  It is intended to match its
behavior almost exactly.  Why implement another compiler that does the
same thing?  The package is useful for a variety of purposes.  It can
be modified more easily than the builtin compiler.  The AST it
generates is useful for analyzing Python source code.

This chapter explains how the various components of the `compiler'
package work.  It blends reference material with a tutorial.

The following modules are part of the `compiler' package:

* Menu:

* basic interface::
* Limitations 2::
* Python Abstract Syntax::
* Using Visitors to Walk ASTs::
* Bytecode Generation::


File: python-lib.info,  Node: basic interface,  Next: Limitations 2,  Prev: Python compiler package,  Up: Python compiler package

The basic interface
===================

The top-level of the package defines four functions.  If you import
`compiler', you will get these functions and a collection of modules
contained in the package.

`parse(buf)'
     Returns an abstract syntax tree for the Python source code in BUF.
     The function raises SyntaxError if there is an error in the source
     code.  The return value is a `compiler.ast.Module' instance that
     contains the tree.

`parseFile(path)'
     Return an abstract syntax tree for the Python source code in the
     file specified by PATH.  It is equivalent to
     `parse(open(PATH).read())'.

`walk(ast, visitor[, verbose])'
     Do a pre-order walk over the abstract syntax tree AST.  Call the
     appropriate method on the VISITOR instance for each node
     encountered.

`compile(source, filename, mode, flags=None,  dont_inherit=None)'
     Compile the string SOURCE, a Python module, statement or
     expression, into a code object that can be executed by the exec
     statement or `eval()'. This function is a replacement for the
     built-in `compile()' function.

     The FILENAME will be used for run-time error messages.

     The MODE must be 'exec' to compile a module, 'single' to compile a
     single (interactive) statement, or 'eval' to compile an expression.

     The FLAGS and DONT_INHERIT arguments affect future-related
     statements, but are not supported yet.

`compileFile(source)'
     Compiles the file SOURCE and generates a .pyc file.

The `compiler' package contains the following modules: `ast', `consts',
`future', `misc', `pyassem', `pycodegen', `symbols', `transformer', and
`visitor'.


File: python-lib.info,  Node: Limitations 2,  Next: Python Abstract Syntax,  Prev: basic interface,  Up: Python compiler package

Limitations
===========

There are some problems with the error checking of the compiler
package.  The interpreter detects syntax errors in two distinct phases.
One set of errors is detected by the interpreter's parser, the other
set by the compiler.  The compiler package relies on the interpreter's
parser, so it get the first phases of error checking for free.  It
implements the second phase itself, and that implement is incomplete.
For example, the compiler package does not raise an error if a name
appears more than once in an argument list: `def f(x, x): ...'

A future version of the compiler should fix these problems.

